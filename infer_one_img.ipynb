{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4656, 4176, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "img = np.array(Image.open(\"/mnt/disk1/tritd/CropFromWeb/snapshot-2024-03-05T00_00_00Z.tif\"))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.PAD_convLSTM import ConvLSTM\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.strategies import DDPStrategy as DDPPlugin\n",
    "# from pytorch_lightning.plugins import DDPPlugin\n",
    "# from pytorch_lightning.strategies import DDPStrategys\n",
    "import torch\n",
    "from utils.settings.config import RANDOM_SEED, CROP_ENCODING, LINEAR_ENCODER, CLASS_WEIGHTS, BANDS\n",
    "run_path = Path(*Path(\"/mnt/disk1/tritd/S4A-Models/logs/convlstm/100epoch/run_20240228195928/checkpoints/epoch=7-step=30240.ckpt\").parts[:-2])\n",
    "model = ConvLSTM.load_from_checkpoint(\"/mnt/disk1/tritd/S4A-Models/logs/convlstm/100epoch/run_20240228195928/checkpoints/epoch=7-step=30240.ckpt\",\n",
    "                                              map_location=torch.device('cpu'),\n",
    "                                              run_path=run_path,\n",
    "                                              linear_encoder=LINEAR_ENCODER,\n",
    "                                              checkpoint_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 61, 61])\n"
     ]
    }
   ],
   "source": [
    "numpy_array = np.random.rand(2, 1, 4, 61, 61)\n",
    "\n",
    "# Converting numpy array to a torch FloatTensor\n",
    "input = torch.FloatTensor(numpy_array).cuda()\n",
    "model = model.cuda()\n",
    "output = model(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first adjusted small array: 5313\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "980\n",
      "1000\n",
      "1020\n",
      "1040\n",
      "1060\n",
      "1080\n",
      "1100\n",
      "1120\n",
      "1140\n",
      "1160\n",
      "1180\n",
      "1200\n",
      "1220\n",
      "1240\n",
      "1260\n",
      "1280\n",
      "1300\n",
      "1320\n",
      "1340\n",
      "1360\n",
      "1380\n",
      "1400\n",
      "1420\n",
      "1440\n",
      "1460\n",
      "1480\n",
      "1500\n",
      "1520\n",
      "1540\n",
      "1560\n",
      "1580\n",
      "1600\n",
      "1620\n",
      "1640\n",
      "1660\n",
      "1680\n",
      "1700\n",
      "1720\n",
      "1740\n",
      "1760\n",
      "1780\n",
      "1800\n",
      "1820\n",
      "1840\n",
      "1860\n",
      "1880\n",
      "1900\n",
      "1920\n",
      "1940\n",
      "1960\n",
      "1980\n",
      "2000\n",
      "2020\n",
      "2040\n",
      "2060\n",
      "2080\n",
      "2100\n",
      "2120\n",
      "2140\n",
      "2160\n",
      "2180\n",
      "2200\n",
      "2220\n",
      "2240\n",
      "2260\n",
      "2280\n",
      "2300\n",
      "2320\n",
      "2340\n",
      "2360\n",
      "2380\n",
      "2400\n",
      "2420\n",
      "2440\n",
      "2460\n",
      "2480\n",
      "2500\n",
      "2520\n",
      "2540\n",
      "2560\n",
      "2580\n",
      "2600\n",
      "2620\n",
      "2640\n",
      "2660\n",
      "2680\n",
      "2700\n",
      "2720\n",
      "2740\n",
      "2760\n",
      "2780\n",
      "2800\n",
      "2820\n",
      "2840\n",
      "2860\n",
      "2880\n",
      "2900\n",
      "2920\n",
      "2940\n",
      "2960\n",
      "2980\n",
      "3000\n",
      "3020\n",
      "3040\n",
      "3060\n",
      "3080\n",
      "3100\n",
      "3120\n",
      "3140\n",
      "3160\n",
      "3180\n",
      "3200\n",
      "3220\n",
      "3240\n",
      "3260\n",
      "3280\n",
      "3300\n",
      "3320\n",
      "3340\n",
      "3360\n",
      "3380\n",
      "3400\n",
      "3420\n",
      "3440\n",
      "3460\n",
      "3480\n",
      "3500\n",
      "3520\n",
      "3540\n",
      "3560\n",
      "3580\n",
      "3600\n",
      "3620\n",
      "3640\n",
      "3660\n",
      "3680\n",
      "3700\n",
      "3720\n",
      "3740\n",
      "3760\n",
      "3780\n",
      "3800\n",
      "3820\n",
      "3840\n",
      "3860\n",
      "3880\n",
      "3900\n",
      "3920\n",
      "3940\n",
      "3960\n",
      "3980\n",
      "4000\n",
      "4020\n",
      "4040\n",
      "4060\n",
      "4080\n",
      "4100\n",
      "4120\n",
      "4140\n",
      "4160\n",
      "4180\n",
      "4200\n",
      "4220\n",
      "4240\n",
      "4260\n",
      "4280\n",
      "4300\n",
      "4320\n",
      "4340\n",
      "4360\n",
      "4380\n",
      "4400\n",
      "4420\n",
      "4440\n",
      "4460\n",
      "4480\n",
      "4500\n",
      "4520\n",
      "4540\n",
      "4560\n",
      "4580\n",
      "4600\n",
      "4620\n",
      "4640\n",
      "4660\n",
      "4680\n",
      "4700\n",
      "4720\n",
      "4740\n",
      "4760\n",
      "4780\n",
      "4800\n",
      "4820\n",
      "4840\n",
      "4860\n",
      "4880\n",
      "4900\n",
      "4920\n",
      "4940\n",
      "4960\n",
      "4980\n",
      "5000\n",
      "5020\n",
      "5040\n",
      "5060\n",
      "5080\n",
      "5100\n",
      "5120\n",
      "5140\n",
      "5160\n",
      "5180\n",
      "5200\n",
      "5220\n",
      "5240\n",
      "5260\n",
      "5280\n",
      "5300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulate an original image array of size (100, 100, 4)\n",
    "original_image = img\n",
    "\n",
    "# Define the size of the target small arrays\n",
    "small_array_shape = (4, 61, 61)\n",
    "\n",
    "# Calculate the number of slices in each dimension, including padding if necessary\n",
    "num_slices_y = np.ceil(original_image.shape[0] / small_array_shape[1]).astype(int)\n",
    "num_slices_x = np.ceil(original_image.shape[1] / small_array_shape[2]).astype(int)\n",
    "\n",
    "# Pad the original image to ensure it can be evenly divided into the target slice sizes\n",
    "padding_y = (num_slices_y * small_array_shape[1]) - original_image.shape[0]\n",
    "padding_x = (num_slices_x * small_array_shape[2]) - original_image.shape[1]\n",
    "padded_large_array = np.pad(original_image, ((0, padding_y), (0, padding_x), (0, 0)), 'constant')\n",
    "\n",
    "# Initialize a list to hold the adjusted small arrays\n",
    "small_arrays = []\n",
    "\n",
    "# Cut the padded array into small arrays and adjust each to the desired shape\n",
    "for i in range(num_slices_y):\n",
    "    for j in range(num_slices_x):\n",
    "        # Extract the small array\n",
    "        small_array = padded_large_array[\n",
    "            i*small_array_shape[1]:(i+1)*small_array_shape[1],\n",
    "            j*small_array_shape[2]:(j+1)*small_array_shape[2],\n",
    "            :\n",
    "        ]\n",
    "        # Transpose the small array to bring the channels to the front: (4, 61, 61)\n",
    "        small_array_transposed = small_array.transpose(2, 0, 1)\n",
    "        # Extend to (1, 1, 4, 61, 61)\n",
    "        small_array_extended_corrected = np.expand_dims(small_array_transposed, axis=0)\n",
    "        small_array_extended_corrected = np.expand_dims(small_array_extended_corrected, axis=0)\n",
    "        small_arrays.append(small_array_extended_corrected)\n",
    "\n",
    "# Optionally, verify the shape of the first corrected small array for confirmation\n",
    "print(\"Shape of the first adjusted small array:\", len(small_arrays))\n",
    "\n",
    "\n",
    "model_outputs = []\n",
    "with torch.no_grad():\n",
    "    model = model.cuda()\n",
    "\n",
    "    for i in range(len(small_arrays)):\n",
    "        input = torch.FloatTensor(small_arrays[i]).cuda()\n",
    "        \n",
    "        output = model(input).argmax(dim=1)\n",
    "        model_outputs.append(output.cpu().numpy())\n",
    "        if i % 20 == 0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4697, 4209)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize an empty array to merge the model outputs back into, matching the padded input array's size\n",
    "merged_output_height = num_slices_y * small_array_shape[1]\n",
    "merged_output_width = num_slices_x * small_array_shape[2]\n",
    "merged_model_output = np.zeros((merged_output_height, merged_output_width))\n",
    "\n",
    "# Merge each model output back into the correct position in the larger array\n",
    "for i in range(num_slices_y):\n",
    "    for j in range(num_slices_x):\n",
    "        start_y = i * small_array_shape[1]\n",
    "        start_x = j * small_array_shape[2]\n",
    "        # Extract the model output slice, removing the extra dimension\n",
    "        # Assuming model_outputs stores numpy arrays; if tensors, use tensor.cpu().detach().numpy() to convert\n",
    "        slice_output = model_outputs[i*num_slices_x + j].squeeze()  # Shape: (61, 61)\n",
    "        # Place the slice in the correct position of the merged array\n",
    "        merged_model_output[start_y:start_y + small_array_shape[1], start_x:start_x + small_array_shape[2]] = slice_output\n",
    "\n",
    "# If the original image needs to be without the added padding, you can trim the merged_model_output here\n",
    "# For now, merged_model_output matches the padded dimensions\n",
    "\n",
    "merged_model_output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(merged_model_output)\n",
    "print(merged_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/disk1/tritd/CropFromWeb/label_snapshot-2024-03-05T00_00_00Z.npy\",\"wb\") as f:\n",
    "    np.save(f, merged_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tifffile\n",
    "\n",
    "# Step 1: Read the original TIFF file\n",
    "with tifffile.TiffFile('/mnt/disk1/tritd/CropFromWeb/snapshot-2024-03-05T00_00_00Z.tif') as tif:\n",
    "    original_data = tif.asarray()\n",
    "    original_metadata = tif.pages[0].tags\n",
    "\n",
    "# Step 2: Assuming your label array is already created and named `label_array`\n",
    "# label_array = your_label_creation_function(original_data)\n",
    "\n",
    "# Step 3 & 4: Copy metadata and modify or add additional metadata as needed\n",
    "metadata_to_keep = {k: v.value for k, v in original_metadata.items() if k in ['ImageDescription', 'DateTime', 'Software']}\n",
    "# Example of adding or modifying metadata\n",
    "metadata_to_keep['ImageDescription'] = 'This is a label file derived from the original image'\n",
    "\n",
    "# Step 5: Save the new TIFF file with the label data and copied/modified metadata\n",
    "with tifffile.TiffWriter('/mnt/disk1/tritd/CropFromWeb/label_snapshot-2024-03-05T00_00_00Z.tif') as tif_writer:\n",
    "    tif_writer.write(merged_model_output, metadata=metadata_to_keep)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
